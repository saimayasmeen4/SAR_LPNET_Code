{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5S4TDT6G1kl0",
        "FB-it5mJEc9f"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDBXjiEM_eFJ",
        "outputId": "ba49741d-8af0-45f4-9bdb-665fb88a6617"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla K80 (UUID: GPU-93972e33-09ac-f92f-6ba1-e4c6807655b2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr_P9t7pm-Ci"
      },
      "source": [
        "# import os\n",
        "# path, dirs, files = next(os.walk('/content/drive/MyDrive/SAR_LPNET_NEW/test_img/Noisy_R'))\n",
        "# file_count = len(files)\n",
        "# print(file_count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLeaHOyp1La0",
        "outputId": "1d0b78d6-5d04-4a6d-dc58-b6d26b91bdb4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3arkkRRCHYij"
      },
      "source": [
        "#!pip uninstall tensorflow-gpu==1.14.0\n",
        "!pip install tensorflow==1.14.0\n",
        "!pip install tensorflow-gpu==1.14.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T6v2_5Y9yVX"
      },
      "source": [
        "# %tensorflow_version 1.14.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE9R9ROl-HjU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSDG7uPOHiOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3db15308-9861-4c1a-fabb-6f2837c75df0"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_k7Lw5REVTG",
        "outputId": "1547d5eb-c4c3-42d1-ac25-263307e6987a"
      },
      "source": [
        "%cd /content/drive/MyDrive/SAR_LPNET_NEW"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1V4PECjAHfjrUjZqJK8ugx2hOODDIuMK9/SAR_LPNET_NEW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **spatial attention**"
      ],
      "metadata": {
        "id": "w-fjakSJ1lIX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB-it5mJEc9f"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgCiVJJkHUnH",
        "outputId": "f9a91e37-275e-493b-90d3-e6dd48aaa552"
      },
      "source": [
        "#!/usr/bin/env python2\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "num_pyramids = 7  # number of pyramid levels\n",
        "num_blocks = 7    # number of recursive blocks\n",
        "num_feature = 32  # number of feature maps\n",
        "num_channels = 1  # number of input's channels \n",
        "\n",
        "\n",
        "# leaky ReLU\n",
        "def lrelu(x, leak = 0.2, name = 'lrelu'):   \n",
        "    with tf.variable_scope(name):\n",
        "         return tf.maximum(x, leak*x, name = name)   \n",
        "\n",
        "\n",
        "\n",
        "######## Laplacian and Gaussian Pyramid ########\n",
        "def lap_split(img,kernel):\n",
        "    with tf.name_scope('split'):\n",
        "        low = tf.nn.conv2d(img, kernel, [1,2,2,1], 'SAME')\n",
        "        low_upsample = tf.nn.conv2d_transpose(low, kernel*4, tf.shape(img), [1,2,2,1])\n",
        "        high = img - low_upsample\n",
        "    return low, high\n",
        "\n",
        "def LaplacianPyramid(img,kernel,n):\n",
        "    levels = []\n",
        "    for i in range(n):\n",
        "        img, high = lap_split(img, kernel)\n",
        "        levels.append(high)\n",
        "    levels.append(img)\n",
        "    return levels[::-1]\n",
        "\n",
        "def GaussianPyramid(img,kernel,n):\n",
        "    levels = []\n",
        "    low = img\n",
        "    for i in range(n):\n",
        "        low = tf.nn.conv2d(low, kernel, [1,2,2,1], 'SAME')\n",
        "        levels.append(low)\n",
        "    return levels[::-1]\n",
        "######## Laplacian and Gaussian Pyramid ######## \n",
        "\n",
        "\n",
        "\n",
        "# create kernel\n",
        "def create_kernel(name, shape, initializer=tf.contrib.layers.xavier_initializer()):\n",
        "    regularizer = tf.contrib.layers.l2_regularizer(scale = 1e-4)\n",
        "    new_variables = tf.get_variable(name=name, shape=shape, initializer=initializer,\n",
        "                                    regularizer=regularizer)\n",
        "    return new_variables\n",
        "\n",
        "\n",
        "# sub network\n",
        "def subnet(images,num_feature):\n",
        "    kernel0 = create_kernel(name='weights_0', shape=[3, 3, num_channels, num_feature])\n",
        "    biases0 = tf.Variable(tf.constant(0.0, shape=[num_feature], dtype=tf.float32), trainable=True, name='biases_0')  \n",
        "  \n",
        "    kernel1 = create_kernel(name='weights_1', shape=[3, 3, num_feature, num_feature])\n",
        "    biases1 = tf.Variable(tf.constant(0.0, shape=[num_feature], dtype=tf.float32), trainable=True, name='biases_1')  \n",
        "\n",
        "    kernel2 = create_kernel(name='weights_2', shape=[1, 1, num_feature, num_feature])\n",
        "    biases2 = tf.Variable(tf.constant(0.0, shape=[num_feature], dtype=tf.float32), trainable=True, name='biases_2')  \n",
        "\n",
        "    kernel3 = create_kernel(name='weights_3', shape=[3, 3, num_feature, num_feature])\n",
        "    biases3 = tf.Variable(tf.constant(0.0, shape=[num_feature], dtype=tf.float32), trainable=True, name='biases_3')  \n",
        "   \n",
        "    kernel4 = create_kernel(name='weights_4', shape=[1, 1, num_feature, num_channels])\n",
        "    biases4 = tf.Variable(tf.constant(0.0, shape=[num_channels], dtype=tf.float32), trainable=True, name='biases_4')  \n",
        "\n",
        "  #  1st layer\n",
        "    with tf.variable_scope('1st_layer'):    \n",
        "         conv0 = tf.nn.conv2d(images, kernel0, [1, 1, 1, 1], padding='SAME')\n",
        "         bias0 = tf.nn.bias_add(conv0, biases0) \n",
        "         bias0 = lrelu(bias0) # leaky ReLU\n",
        "\n",
        "         out_block =  bias0\n",
        "\n",
        "  #  recursive blocks. First 4 blocks are of SB. Last block is for FEB\n",
        "    for i in range(num_blocks):\n",
        "        with tf.variable_scope('block_%s'%(i+1)):\n",
        "             conv1 = tf.nn.conv2d(out_block, kernel1, [1, 1, 1, 1], padding='SAME')\n",
        "             bias1 = tf.nn.bias_add(conv1, biases1) \n",
        "             bias1 = lrelu(bias1) \n",
        "  \n",
        "             conv2 = tf.nn.conv2d(bias1, kernel2, [1, 1, 1, 1], padding='SAME')\n",
        "             bias2 = tf.nn.bias_add(conv2, biases2) \n",
        "             bias2 = lrelu(bias2) \n",
        "  \n",
        "             conv3 = tf.nn.conv2d(bias2, kernel3, [1, 1, 1, 1], padding='SAME')\n",
        "             bias3 = tf.nn.bias_add(conv3, biases3) \n",
        "             bias3 = lrelu(bias3) \n",
        "\n",
        " \n",
        "\n",
        "            #  out_block = bias3\n",
        "             out_block = tf.add(bias3, bias0) #  shortcut\n",
        "\n",
        "    # conv1 -> conv2 -> conv3 (dilation) -> conv1 -> conv2 -> conv3 (dilation) -> conv1 -> conv2 -> conv3 (dilation) -> conv1 -> conv2 -> conv3 (dilation)\n",
        "    \n",
        "    # Part of FEB\n",
        "    kernel_feb = create_kernel(name='kernel_attention', shape=[1, 1, num_feature, num_channels])\n",
        "    # biases_attention = tf.Variable(tf.constant(0.0, shape=[num_channels], dtype=tf.float32), trainable=True, name='biases_attention') \n",
        "    out_block = tf.nn.conv2d(out_block, kernel_feb, [1, 1, 1, 1], padding='SAME')\n",
        "    conv1_out_block = out_block\n",
        "    # bias_attention = tf.nn.bias_add(conv1, biases_attention) \n",
        "\n",
        "    out_block = tf.concat([out_block, images], -1)\n",
        "    out_block = tf.nn.tanh(out_block, name ='tanh')\n",
        "\n",
        "    # attention block\n",
        "    kernel_attention_2 = create_kernel(name='kernel_attention_2', shape=[1, 1, num_channels*2, num_channels])\n",
        "    biases_attention_2 = tf.Variable(tf.constant(0.0, shape=[num_channels], dtype=tf.float32), trainable=True, name='biases_attention_2') \n",
        "    out_block = tf.nn.conv2d(out_block, kernel_attention_2, [1, 1, 1, 1], padding='SAME')\n",
        "    out_block = tf.nn.bias_add(out_block, biases_attention_2) \n",
        "\n",
        "    \n",
        "\n",
        "    out_block = tf.multiply(out_block, conv1_out_block)\n",
        "\n",
        "   #  reconstruction layer attention\n",
        "    with tf.variable_scope('recons'):    \n",
        "        #  conv = tf.nn.conv2d(out_block, kernel4, [1, 1, 1, 1], padding='SAME')\n",
        "        #  recons = tf.nn.bias_add(conv, biases4) \n",
        "\n",
        "         final_out = tf.add(out_block, images) #  shortcut\n",
        "  \n",
        "    return final_out\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        " \n",
        "# LPNet structure\n",
        "def inference(images):\n",
        "    with tf.variable_scope('inference', reuse=tf.AUTO_REUSE):\n",
        "\n",
        "         k = np.float32([.0625, .25, .375, .25, .0625]) # Gaussian kernel for image pyramid\n",
        "         k = np.outer(k, k) \n",
        "         kernel = k[:,:,None,None]/k.sum()*np.eye(num_channels, dtype=np.float32)\n",
        "         \n",
        "         pyramid = LaplacianPyramid(images, kernel, (num_pyramids - 1)) # rainy Laplacian pyramid\n",
        "\n",
        "         \n",
        "       # subnet 1\n",
        "         with tf.variable_scope('subnet1'):  \n",
        "              out1 = subnet( pyramid[0], int((num_feature)/16) )\n",
        "              out1 = tf.nn.relu(out1) \n",
        "              out1_t = tf.nn.conv2d_transpose(out1, kernel*4, tf.shape(pyramid[1]), [1,2,2,1])\n",
        "           \n",
        "       # subnet 2\n",
        "         with tf.variable_scope('subnet2'):  \n",
        "              out2 = subnet( pyramid[1], int((num_feature)/8) )\n",
        "              out2 = tf.add(out2, out1_t)\n",
        "              out2 = tf.nn.relu(out2) \n",
        "              out2_t = tf.nn.conv2d_transpose(out2, kernel*4, tf.shape(pyramid[2]), [1,2,2,1])\n",
        "\n",
        "       # subnet 3\n",
        "         with tf.variable_scope('subnet3'):  \n",
        "              out3 = subnet( pyramid[2], int((num_feature)/4) )\n",
        "              out3 = tf.add(out3, out2_t)\n",
        "              out3 = tf.nn.relu(out3) \n",
        "              out3_t = tf.nn.conv2d_transpose(out3, kernel*4, tf.shape(pyramid[3]), [1,2,2,1])\n",
        "     \n",
        "       # subnet 4\n",
        "         with tf.variable_scope('subnet4'):  \n",
        "              out4 = subnet( pyramid[3], int((num_feature)/2) )\n",
        "              out4 = tf.add(out4, out3_t)\n",
        "              out4 = tf.nn.relu(out4) \n",
        "              out4_t = tf.nn.conv2d_transpose(out4, kernel*4, tf.shape(pyramid[4]), [1,2,2,1])\n",
        "     \n",
        "       # subnet 5\n",
        "         with tf.variable_scope('subnet5'):  \n",
        "              out5 = subnet( pyramid[4], int(num_feature) )\n",
        "              out5 = tf.add(out5, out4_t)\n",
        "              out5 = tf.nn.relu(out5) \n",
        "              \n",
        "         outout_pyramid = []        \n",
        "         outout_pyramid.append(out1)\n",
        "         outout_pyramid.append(out2)\n",
        "         outout_pyramid.append(out3)\n",
        "         outout_pyramid.append(out4)\n",
        "         outout_pyramid.append(out5)\n",
        "         \n",
        "         return outout_pyramid\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.reset_default_graph()   \n",
        "    \n",
        "    input_x = tf.placeholder(tf.float32, [10,None,None,num_channels])\n",
        "    \n",
        "    outout_pyramid  = inference(input_x)\n",
        "    var_list = tf.trainable_variables()   \n",
        "    print(\"Total parameters' number: %d\" \n",
        "         %(np.sum([np.prod(v.get_shape().as_list()) for v in var_list])))  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters' number: 26866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hqcszlXH9ui"
      },
      "source": [
        "# import os\n",
        "\n",
        "# path, dirs, files = next(os.walk('/content/drive/MyDrive/SAR/virtual_sar_training_set/noisy_1c_full/'))\n",
        "# file_count = len(files)\n",
        "# print(file_count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlrlghGSGSzp"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3RW6Q3YHv4P"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm8Wv6Z3Nbo1"
      },
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "# from model import inference, GaussianPyramid \n",
        "\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "\n",
        "num_pyramids = 5       # number of pyramid levels\n",
        "learning_rate = 1e-3   # learning rate\n",
        "iterations = int(24000)  # iterations\n",
        "batch_size = 16        # batch size\n",
        "num_channels = 1       # number of input's channels \n",
        "patch_size = 80        # patch size \n",
        "save_model_path = './model/realmodel/sp'  # path of saved model\n",
        "model_name = 'model-epoch'    # name of saved model\n",
        "\n",
        "input_path = '../SAR_LPNET_NEW/inp/' # Noisy images\n",
        "gt_path = '../SAR_LPNET_NEW/lab/'    # ground truth  \n",
        "\n",
        "# randomly select image patches\n",
        "def _parse_function(input_path, gt_path, patch_size = patch_size):   \n",
        "    image_string = tf.read_file(input_path)  \n",
        "    image_decoded = tf.image.decode_png(image_string, channels=num_channels)  \n",
        "    rainy = tf.cast(image_decoded, tf.float32)/255.0\n",
        "          \n",
        "    image_string = tf.read_file(gt_path)  \n",
        "    image_decoded = tf.image.decode_png(image_string, channels=num_channels)  \n",
        "    label = tf.cast(image_decoded, tf.float32)/255.0\n",
        "          \n",
        "    t = time.time()\n",
        "    Data = tf.random_crop(rainy, [patch_size, patch_size ,num_channels],seed = t)   # randomly select patch\n",
        "    Label = tf.random_crop(label, [patch_size, patch_size ,num_channels],seed = t)       \n",
        "    return Data, Label \n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':    \n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    input_files = os.listdir(input_path)\n",
        "    label_files = os.listdir(gt_path)       \n",
        "\n",
        "    for i in range(len(input_files)):\n",
        "        filename = input_files[i]\n",
        "\n",
        "        input_files[i] = input_path + filename\n",
        "        label_files[i] = gt_path + filename \n",
        "            \n",
        "    print(len(input_files),input_files )\n",
        "    print(len(label_files),label_files)\n",
        "\n",
        "    input_files = tf.convert_to_tensor(input_files, dtype=tf.string)  \n",
        "    label_files = tf.convert_to_tensor(label_files, dtype=tf.string)  \n",
        "     \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_files, label_files))\n",
        "    dataset = dataset.map(_parse_function)    \n",
        "    dataset = dataset.prefetch(buffer_size=batch_size * 10)\n",
        "    dataset = dataset.batch(batch_size).repeat()  \n",
        "    iterator = dataset.make_one_shot_iterator()   \n",
        "    inputs, labels = iterator.get_next()\n",
        "    inputs, labels = iterator.get_next()\n",
        "\n",
        "\n",
        "\n",
        "    k = np.float32([.0625, .25, .375, .25, .0625])  # Gaussian kernel for image pyramid\n",
        "    k = np.outer(k, k) \n",
        "    kernel = k[:,:,None,None]/k.sum()*np.eye(num_channels, dtype = np.float32)\n",
        "    labels_GaussianPyramid = GaussianPyramid( labels, kernel, (num_pyramids-1) ) # Gaussian pyramid for ground truth\n",
        "\n",
        "    outout_pyramid = inference(inputs) # LPNet\n",
        "\n",
        "    loss1 = tf.reduce_mean(tf.abs(outout_pyramid[0] - labels_GaussianPyramid[0]))    # L1 loss\n",
        "    loss2 = tf.reduce_mean(tf.abs(outout_pyramid[1] - labels_GaussianPyramid[1]))    # L1 loss\n",
        "    loss3 = tf.reduce_mean(tf.abs(outout_pyramid[2] - labels_GaussianPyramid[2]))    # L1 loss\n",
        "  \n",
        "    loss41 = tf.reduce_mean(tf.abs(outout_pyramid[3] - labels_GaussianPyramid[3]))   # L1 loss\n",
        "    loss42 = tf.reduce_mean((1. - tf.image.ssim(outout_pyramid[3],labels_GaussianPyramid[3], max_val=1.0))/2.) # SSIM loss\n",
        "\n",
        "    loss51 = tf.reduce_mean(tf.abs(outout_pyramid[4] - labels))  # L1 loss\n",
        "    loss52 = tf.reduce_mean((1. - tf.image.ssim(outout_pyramid[4],labels, max_val=1.0))/2.) # SSIM loss\n",
        "    \n",
        "    loss = loss1 + loss2 + loss3 + loss41 + loss42 + loss51 + loss52\n",
        "    g_optim =  tf.train.AdamOptimizer(learning_rate).minimize(loss) # Optimization method: Adam\n",
        "    \n",
        "    all_vars = tf.trainable_variables()  \n",
        "    saver = tf.train.Saver(var_list=all_vars, max_to_keep = 1000)  \n",
        "\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth=True\n",
        "    with tf.Session(config=config) as sess:\n",
        "      \n",
        "       sess.run(tf.group(tf.global_variables_initializer(), \n",
        "                         tf.local_variables_initializer()))\n",
        "       tf.get_default_graph().finalize()\t\n",
        "              \n",
        "       if tf.train.get_checkpoint_state(save_model_path):   # load previous trained model \n",
        "          ckpt = tf.train.latest_checkpoint(save_model_path)\n",
        "          saver.restore(sess, ckpt)  \n",
        "          ckpt_num = re.findall(r'(\\w*[0-9]+)\\w*',ckpt)\n",
        "          start_point = int(ckpt_num[0]) + 1     \n",
        "          print(\"loaded successfully\")\n",
        "       else:  # re-training when no models found\n",
        "          print(\"re-training\")\n",
        "          start_point = 0  \n",
        "          \n",
        "       check_input, check_label =  sess.run([inputs,labels])\n",
        "       print(\"check patch pair:\")  \n",
        "       plt.subplot(1,3,1)     \n",
        "       plt.imshow(np.squeeze(check_input[0,:,:,:]), cmap='gray')\n",
        "       plt.title('input')         \n",
        "       plt.subplot(1,3,2)    \n",
        "       plt.imshow(np.squeeze(check_label[0,:,:,:]), cmap='gray')\n",
        "       plt.title('ground truth')      \n",
        "       plt.show()    \n",
        "\n",
        "        \n",
        "       start = time.time()\n",
        "     \n",
        "       \n",
        "       for j in range(start_point,iterations):    \n",
        "     \n",
        "           _, Training_Loss = sess.run([g_optim,loss])  # training\n",
        "           end = time.time() \n",
        "\n",
        "           print ('%d / %d iteraions, Training Loss  = %.4f, running time = %.1f s' \n",
        "                  % (j+1, iterations, Training_Loss, (end-start)))\n",
        "           \n",
        "           if np.mod(j+1,3000) == 0 and j != 0:   \n",
        "              end = time.time() \n",
        "              print (\"Saving model\")\n",
        "\n",
        "              # print('j',j,'loss',np.average(losss))\n",
        "             \n",
        "              print ('%d / %d iteraions, Training Loss  = %.4f, running time = %.1f s' \n",
        "                     % (j+1, iterations, Training_Loss, (end-start)))\n",
        "              save_model_path = os.path.abspath(save_model_path)          \n",
        "              save_path_full = os.path.join(save_model_path, model_name) \n",
        "              saver.save(sess, save_path_full, global_step = j+1) # save model every 100 iterations\n",
        "              start = time.time()  \n",
        "              \n",
        "       print ('training finished') \n",
        "    sess.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQNfxie6USvE"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSmKEqF3HeWj"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDNdQ7ovHk_L"
      },
      "source": [
        "\n",
        "import os\n",
        "import skimage.io\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import math\n",
        "# import model\n",
        "\n",
        "from skimage.metrics import structural_similarity  as ssim\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# model_path = './model/pre_trained/with_attention_R'\n",
        "# pre_trained_model_path = '/content/drive/MyDrive/SAR_LPNET_NEW/model/pre_trained/with_attention_R/model-epoch-30000'\n",
        "model_path = './model/pre_trained/with_attention_Real32'\n",
        "pre_trained_model_path = '/content/drive/MyDrive/SAR_LPNET_NEW/model/realmodel/sp/model-epoch-24000'\n",
        "\n",
        "img_path = '../SAR_LPNET_NEW/inp/'\n",
        "# img_path = '../virtual_sar_training_set/noisy_1c/'\n",
        "results_path = './test_img/rez/' # the path of de-noised images\n",
        "\n",
        "psnr_list = [] \n",
        "\n",
        "def _parse_function(filename):   \n",
        "  image_string = tf.read_file(filename)  \n",
        "  image_decoded = tf.image.decode_png(image_string, channels=num_channels)  \n",
        "  rainy = tf.cast(image_decoded, tf.float32)/255.0   \n",
        "  return rainy \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   imgName = os.listdir(img_path)\n",
        "   num_img = len(imgName)\n",
        "   \n",
        "   whole_path = []\n",
        "   for i in range(num_img):\n",
        "      whole_path.append(img_path + imgName[i])\n",
        "      \n",
        "    \n",
        "   filename_tensor = tf.convert_to_tensor(whole_path, dtype=tf.string)     \n",
        "   dataset = tf.data.Dataset.from_tensor_slices((filename_tensor))\n",
        "   dataset = dataset.map(_parse_function)    \n",
        "   dataset = dataset.prefetch(buffer_size=10)\n",
        "   dataset = dataset.batch(batch_size=1).repeat()  \n",
        "   iterator = dataset.make_one_shot_iterator()\n",
        "   \n",
        "   rain = iterator.get_next()  \n",
        " \n",
        "   pyramid = inference(rain)\n",
        "   output = tf.clip_by_value(pyramid[-1], 0., 1.)\n",
        "   output = output[0,:,:,:]\n",
        "\n",
        "   config = tf.ConfigProto()\n",
        "   config.gpu_options.allow_growth=False   \n",
        "   saver = tf.train.Saver()\n",
        "   \n",
        "   total_ssim = 0\n",
        "   total_psnr = 0\n",
        "   with tf.Session(config=config) as sess:\n",
        "      with tf.device('/gpu:0'): \n",
        "        #   if tf.train.get_checkpoint_state(model_path):  \n",
        "        #       ckpt = tf.train.latest_checkpoint(model_path)  # try your own model \n",
        "        #       saver.restore(sess, ckpt)\n",
        "        #       print (\"Loading model\")\n",
        "        #   else:\n",
        "          saver.restore(sess, pre_trained_model_path) # try a pre-trained model \n",
        "          print (\"Loading pre-trained model\")\n",
        "\n",
        "          for i in tqdm_notebook(range(num_img)):     \n",
        "             derained, ori = sess.run([output, rain])              \n",
        "             derained = np.uint8(derained* 255.)\n",
        "             ori = np.uint8(ori* 255.)\n",
        "\n",
        "             index = imgName[i].rfind('.')\n",
        "             name = imgName[i][:index]\n",
        "             skimage.io.imsave(results_path + name +'.png', derained)\n",
        "            #  print('%d / %d images processed' % (i+1,num_img))\n",
        "            #  plt.subplot(1,2,1)     \n",
        "            #  plt.imshow(np.squeeze(ori[0,:,:,:]), cmap='gray')          \n",
        "            #  plt.title('Noisy')\n",
        "            #  plt.subplot(1,2,2)    \n",
        "            #  plt.imshow(np.squeeze(derained), cmap='gray')     \n",
        "            #  plt.title('Denoised')\n",
        "            #  plt.show()\n",
        "\n",
        "            #  im1 = tf.decode_png('path/to/im1.png')\n",
        "            #  im2 = tf.decode_png('path/to/im2.png')\n",
        "            #  # Compute PSNR over tf.uint8 Tensors.\n",
        "            #  psnr1 = tf.image.psnr(im1, im2, max_val=255)            \n",
        "            #  im1 = tf.image.convert_image_dtype(ori[0,:,:,:], tf.float32)\n",
        "            #  im2 = tf.image.convert_image_dtype(derained, tf.float32)\n",
        "            #  psnr2 = tf.image.psnr(im1, im2, max_val=1.0)\n",
        "            # #  psnr1 = tf.image.psnr(ori, derained, max_val=255, name=None)\n",
        "            #  logs[i] = PSNR(ori, derained)\n",
        "            #  print(logs)\n",
        "             s = ssim(ori[0,:,:,:], derained, multichannel=True)\n",
        "            #  print('ssim =', s)\n",
        "             psnr1 = tf.image.psnr(ori[0,:,:,:], derained, max_val=255, name=None)\n",
        "            #  print('psnr =', psnr1)  \n",
        "             total_ssim+=s\n",
        "             total_psnr+=psnr1\n",
        "            #  mse=tf.reduce_mean( (img1 - img2) ** 2 )\n",
        "            #  print(20 * np.log10(1 / (mse ** 0.5)))\n",
        "            #  break\n",
        "           \n",
        "              \n",
        "      print('All done')\n",
        "\n",
        "      print(\"Average ssim:\", total_ssim/num_img)\n",
        "      print(\"Average psnr:\", total_psnr.eval()/num_img)\n",
        "   sess.close()   \n",
        "   \n",
        "   psnr1 = tf.image.psnr(ori[0,:,:,:], derained, max_val=255, name=None)\n",
        "   print((psnr1))      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO8G0OCZcnN6"
      },
      "source": [
        "plt.subplot(1,2,1)     \n",
        "plt.imshow(ori[0,:,:,0],cmap='gray')          \n",
        "plt.title('Noisy')\n",
        "plt.subplot(1,2,2)    \n",
        "plt.imshow(derained[:,:,0],cmap='gray')\n",
        "plt.title('denoised')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVAN7xoIEc4H"
      },
      "source": [
        "plt.subplot(1,2,1)     \n",
        "plt.imshow(ori[0,:,:,0],cmap='gray')          \n",
        "plt.title('Noisy')\n",
        "plt.subplot(1,2,2)    \n",
        "plt.imshow(derained[:,:,0],cmap='gray')\n",
        "plt.title('denoised')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZwme849ZhvX"
      },
      "source": [
        "with tf.Session(config=config) as sess:\n",
        "    print(psnr1.eval())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9_h-sP9mTrK"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "images= os.listdir('/content/drive/MyDrive/SAR_LPNET_NEW/test_img/results_with_attention_R_50 epoch/')\n",
        "print(len(images))\n",
        "print((images))\n",
        "for i in range(len(images)):\n",
        "  img = cv2.imread('/content/drive/MyDrive/SAR_LPNET_NEW/test_img/results_with_attention_R_50 epoch/'+images[i],0)\n",
        "  name=images[i]\n",
        "  print(name)\n",
        "  gauss = np.random.normal(0.03,0.04,img.size) #base test\n",
        "  # gauss = np.random.normal(0.3,0.6,img.size) proposed\n",
        "  gauss = gauss.reshape(img.shape[0],img.shape[1]).astype('uint8')\n",
        "  noise = img + img * gauss\n",
        "  plt.imshow(noise[:,:],cmap='gray')\n",
        "  plt.show()\n",
        "  \n",
        "  # cv2.imwrite(os.path.join('/content/drive/MyDrive/SAR_LPNET_NEW/test_img/487noisy' , name), noise)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "  \n",
        "# Save image in set directory\n",
        "# Read RGB image\n",
        "img = cv2.imread('/content/12000.png') \n",
        "  \n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "img = cv2.imread('/content/12000.png') \n",
        "cv2_imshow(img)\n",
        "height, width, channels = img.shape\n",
        "print(height, width, channels)"
      ],
      "metadata": {
        "id": "9FErxf2lUF5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "img = Image.open('/content/12000.png')\n",
        "\n",
        "img2 = Image.open('/content/image (2).png')\n",
        "\n",
        "Nimg = img.resize((256,256))   # image resizing\n",
        "print('Nimg',Nimg)\n",
        "Nimg2 = img2.resize((256,256))\n",
        "\n",
        "print('Nimg',Nimg2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcXIyZreTolN",
        "outputId": "1ded8a40-e068-4e3c-ccf3-cb4aa9f4ce05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nimg <PIL.Image.Image image mode=L size=256x256 at 0x7F5BBFA10410>\n",
            "Nimg <PIL.Image.Image image mode=RGBA size=256x256 at 0x7F5BBFA89ED0>\n"
          ]
        }
      ]
    }
  ]
}